{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from numpy.linalg import norm\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "lines_test = []\n",
    "\n",
    "categories = [1, 2, 3, 4, 5]\n",
    "with open(\"train.dat\", \"r\") as fh:\n",
    "    lines = fh.readlines()\n",
    "    \n",
    "lines_train = []\n",
    "classes = []\n",
    "for s in lines:\n",
    "    temp = s.split(\"\\t\")\n",
    "    classes.append(temp[0])\n",
    "    lines_train.append(temp[1])\n",
    "    \n",
    "with open(\"test.dat\", \"r\") as fh:\n",
    "    lines_test = fh.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14442\n",
      "14438\n"
     ]
    }
   ],
   "source": [
    "print(len(lines_test))\n",
    "# print(classes)\n",
    "print(len(lines_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "# train1 = [l.split() for l in lines_train]\n",
    "# test1 = [l.split() for l in lines_test]\n",
    "\n",
    "\n",
    "def filterLen(docs, minlen):\n",
    "    r\"\"\" filter out terms that are too short. \n",
    "    docs is a list of lists, each inner list is a document represented as a list of words\n",
    "    minlen is the minimum length of the word to keep\n",
    "    \"\"\"\n",
    "#     return [ [t for t in d if len(t) >= minlen ] for d in docs ]\n",
    "    temp = \"\"\n",
    "    for d in docs:\n",
    "#         d = d.sub('[^A-Za-z0-9]+', '', string)\n",
    "#         d = ''.join(e for e in d if e.isalnum())\n",
    "#         temp+=d+\" \"\n",
    "        if len(d)>minlen:\n",
    "            temp+=d+\" \"\n",
    "    return temp;\n",
    "#     return [d for d in docs if len(d)>minlen]\n",
    "\n",
    "\n",
    "\n",
    "def removeUnwantedCharacters(data):\n",
    "    docs_temp = []\n",
    "    for l in data:\n",
    "        temp = l.split()\n",
    "        docs_temp.append(filterLen(temp, 4))\n",
    "    return docs_temp\n",
    "\n",
    "docs_train = removeUnwantedCharacters(lines_train)\n",
    "docs_test = removeUnwantedCharacters(lines_test)\n",
    "\n",
    "print(type(docs_train))\n",
    "\n",
    "# docs_train = filterLen(train1, 4)\n",
    "# docs_test = filterLen(test1, 4)\n",
    "# print(docs_train[:5][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "<type 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(docs_train))\n",
    "\n",
    "print(type(docs_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14438, 1048576)\n",
      "['4' '5' '2' ..., '1' '2' '3']\n",
      "14438\n"
     ]
    }
   ],
   "source": [
    "vectorizer = HashingVectorizer(stop_words='english')\n",
    "X_train = vectorizer.transform(docs_train)\n",
    "\n",
    "# vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "# X_train = vectorizer.fit_transform(docs_train)\n",
    "print(X_train.shape)\n",
    "\n",
    "y_train = np.asarray(classes)\n",
    "print(y_train)\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(docs_test)\n",
    "# print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target_names = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "def classify(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "    print(type(pred))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=120, p=2,\n",
      "           weights='uniform')\n",
      "train time: 0.007s\n",
      "test time:  15.781s\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "pred1 = classify(KNeighborsClassifier(n_neighbors=(int(len(lines_train)**0.5))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5' '5' '5' '2' '1' '4' '5' '5' '1' '5']\n"
     ]
    }
   ],
   "source": [
    "print(pred1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=115, p=2,\n",
      "           weights='uniform')\n",
      "train time: 0.009s\n",
      "test time:  14.664s\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "pred2 = classify(KNeighborsClassifier(n_neighbors=115))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5' '5' '5' '2' '1' '4' '1' '5' '1' '5']\n"
     ]
    }
   ],
   "source": [
    "print(pred2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=50,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 2.302s\n",
      "test time:  0.048s\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# pred3 = SGDClassifier(alpha=.0001, n_iter=50, penalty=penalty)\n",
    "pred3 = classify(SGDClassifier(alpha=.0001, n_iter=50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5' '5' '5' '5' '1' '3' '5' '5' '1' '4']\n"
     ]
    }
   ],
   "source": [
    "print(pred3[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
      "              fit_intercept=True, loss='hinge', max_iter=None, n_iter=None,\n",
      "              n_jobs=1, random_state=None, shuffle=True, tol=None,\n",
      "              verbose=0, warm_start=False)\n",
      "train time: 0.270s\n",
      "test time:  0.033s\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "# pred4 = classify(PassiveAggressiveClassifier(n_iter=50))\n",
    "pred4 = classify(PassiveAggressiveClassifier())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '5' '1' '5' '1' '3' '5' '5' '1' '3' '3' '4' '4' '2' '2' '1' '3' '1'\n",
      " '4' '4' '3' '1' '5' '4' '1' '1' '4' '4' '1' '3']\n"
     ]
    }
   ],
   "source": [
    "print(pred4[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findMatchPercentage(p1, p2):\n",
    "#     p1 = p1.tolist()\n",
    "#     p2 = p2.tolist()\n",
    "    match = 0\n",
    "    total = len(p1)\n",
    "    for i in range(0,total-1):                \n",
    "        if(p1[i]==p2[i]):\n",
    "            match = match+1\n",
    "    per = ((float(match)/float(total))*100)\n",
    "    return per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.0473618612\n"
     ]
    }
   ],
   "source": [
    "print(findMatchPercentage(pred1, pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeToFile(pred):\n",
    "    with open('output3.dat','w+') as f:\n",
    "        for p in pred:\n",
    "            f.write(str(p)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writeToFile(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readFile(filename):\n",
    "    content = []\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "        content = [x.strip() for x in content] \n",
    "    return content\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.8530674422\n"
     ]
    }
   ],
   "source": [
    "p1 = readFile(\"output4.dat\")\n",
    "print(findMatchPercentage(p1, pred1.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
